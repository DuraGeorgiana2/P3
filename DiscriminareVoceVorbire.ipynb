{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Instalare module si importare librarii"
      ],
      "metadata": {
        "id": "gABvxNzyx-J7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pydub\n",
        "%pip install -q tfds-nightly tensorflow matplotlib\n",
        "%pip install tensorflow-datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHJnCCMHyCd0",
        "outputId": "8008d66e-37e4-4f88-9957-0fa8d4169fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.25.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.4.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.1)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.4.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.10.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2024.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.63.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importare librarii necesare\n"
      ],
      "metadata": {
        "id": "m4MerTU2yKP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np                                    # pt lucrul cu arrays\n",
        "import pandas as pd                                   # toolkit pt data analysys\n",
        "import os\n",
        "import pathlib                                        # pt a interactiona usor cu filesystem\n",
        "import matplotlib.pyplot as plt                       # pt plotare\n",
        "import seaborn as sns                                 # bazata pe matplotlib, pt a vizualiza date\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split  # pt a imparti in testing si training data\n",
        "import tensorflow as tf                               # pt a crea modele\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "HZyQ1u4IyM_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectare cu Google Drive"
      ],
      "metadata": {
        "id": "VlJ7KPEryQ76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5jayKviyTdu",
        "outputId": "daffc834-3ed6-4201-c455-86f9b7a9c72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fișierele audio sunt organizate în directoare diferite pentru fiecare clasă (speech/music).\n",
        "\n",
        "Catgorii: music_wav, speech_wav"
      ],
      "metadata": {
        "id": "eY3ybDizyX6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir ='/content/drive/MyDrive/PS/PS- proiect'                          # Specificam directorul in care se afla datele de antrenament, validare și testare\n",
        "categories = np.array(tf.io.gfile.listdir(str(data_dir)))                 # listam fisierele și directoarele din directorul specificat -> obiect de tip \"list\"\n",
        "categories = [category for category in categories if 'wav' in category]   # se folosește o listă comprimată pentru a filtra numai directoarele care conțin fișiere de tipul \"wav\", adică fișiere audio\n",
        "                                                                          # Aceste directoare vor fi clasificările/etichetele (categories) pentru model.\n",
        "print('Categorii: ',categories)                                           # Afisam lista de categorii/etichete"
      ],
      "metadata": {
        "id": "oO-P0tsPyc7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prelucarea setului de date de fisiere audio\n",
        "\n",
        "Funcția \"glob\" din TensorFlow I/O : pentu a căuta toate fișierele din directorul dat, precum și din toate subdirectoarele sale. Rezultatul este o listă de nume de fișiere.\n",
        "\n",
        "filtram această listă pentru a include doar fișierele care conțin cuvântul \"wav\" în numele lor, folosind o listă de comprehensiune.\n",
        "\n",
        "Se amestecă aleator lista rezultată, folosind funcția \"shuffle\" din TensorFlow.\n",
        "\n",
        "Se calculează numărul total de exemple din lista de fișiere și se afișează"
      ],
      "metadata": {
        "id": "M9VCtlZXyi-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
        "filenames = [filename for filename in filenames if 'wav' in filename]\n",
        "filenames = tf.random.shuffle(filenames)\n",
        "num_samples = len(filenames)\n",
        "\n",
        "print('Numarul total de fisiere audio:', num_samples)\n",
        "print('Numarul de fisiere audio per label:', len(tf.io.gfile.listdir(str(data_dir + '/' + categories[0]))))\n",
        "print('Tensorul unui fisier exemplu:', filenames[0])"
      ],
      "metadata": {
        "id": "_lM1Va_9ykHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setul de date este impartit in set de antrenare (96 fisiere audio), validare (16) si testare (16)"
      ],
      "metadata": {
        "id": "0RLDQdCOy7tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 0.75\n",
        "\n",
        "train_files = filenames[:int(len(filenames) * TRAIN_SIZE)]                         # 0 -> 95 :  set de antrenare\n",
        "val_files = filenames[int(len(filenames) * TRAIN_SIZE):int(len(filenames)*0.875)]  # 96 -> 111 : set de validare => 16 fisiere audio\n",
        "test_files = filenames[int(len(filenames)*0.875):]                                 # 112 -> 128 set de test => 16\n",
        "\n",
        "print('Dimensiunea setului de antrenare:', len(train_files))\n",
        "print('Dimensiunea setului de validare:', len(val_files))\n",
        "print('Dimensiunea setului de test:', len(test_files))"
      ],
      "metadata": {
        "id": "LMXTqKuoy--U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_files"
      ],
      "metadata": {
        "id": "VGTvt338zD0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_files"
      ],
      "metadata": {
        "id": "lbwuT1aWzI3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_files"
      ],
      "metadata": {
        "id": "eLFgTHizzKnd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}